if(Sys.info()["user"]=="fiona" ) {setwd("/Users/fiona/Dropbox (Princeton)/BBH/BBH1/")}
#Load data
auto <- read_excel("data/03_final/issues_texts/auto.xlsx")
#Create climate binary variable
auto$CLI <- grepl("ENV|CAW|ENG|FUE", auto$issue_code)
#Filter data for climate lobbying only
auto <- auto %>%
filter(CLI == "TRUE")
auto <- auto %>%
filter(
complete.cases(
op_expo_ew, rg_expo_ew, ph_expo_ew
)
)
View(auto)
View(auto)
#Create corpus
auto_corpus <-corpus(auto$issue_text, docnames = seq_len(nrow(auto)))
# Add document-level information
docvars(auto_corpus) <- auto[, c("gvkey", "conm", "year", "op_expo_ew", "rg_expo_ew", "ph_expo_ew")]
##Topic model exploration
auto_dfm <- dfm(auto_corpus,
remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))
auto_dfm <- dfm_trim(auto_dfm, min_termfreq = 4, max_docfreq = 10)
set.seed(100)
if (require(stm)) {
my_lda_fit20 <- stm(auto_dfm, K = 20, verbose = FALSE)
plot(my_lda_fit20)
}
#explore dfm
topfeatures(auto_dfm, 20)  # 20 top words
keywords_opp <- c("incentiv", "subsid", "tax credit", "charging", "technology", "electric", "funding", "research", "development", "battery", "hybrid", "zero emissions")
keywords_reg <- c("standard", "rule", "efficien", "CAFE", "fuel economy", "harmoniz", "implement", "regulat", "monitor", "carbon price", "target", "restriction")
keywords_phy <- c("greenhouse gas", "water", "climate change", "general", "global warming", "heat", "environment", "land", "nature", "air", "pollution", "flood" )
#Calculate overall frequency
keywords_opp_frequency <- textstat_frequency(auto_corpus, pattern = keywords_opp)
install.packages("quanteda.textstats")
# Load packages
pacman::p_load(tidyverse, readxl, quanteda, quanteda.textstats, stm)
#Calculate overall frequency
keyword__opp_freq <- textstat_frequency(auto_corpus, pattern = keywords_opp)
##Make dfm
toks_auto <- tokens(data_auto, remove_punct = TRUE)
##Make dfm
toks_auto <- tokens(auto_corpus, remove_punct = TRUE)
head(toks_auto)
dfm_auto <- dfm(toks_auto)
print(dfm_auto)
#Calculate overall frequency
keyword__opp_freq <- textstat_frequency(dfm_auto, pattern = keywords_opp)
# Create a Keyword-In-Context object for each keyword
kwic_list <- lapply(keywords_opp, function(keyword) {
kwic(dfm_auto, keyword)
})
# Create a Keyword-In-Context object for each keyword
keyword_freq_opp <- sapply(keywords_opp, function(keyword) {
sum(toks_auto == keyword)
})
print(keyword_freq_opp)
View(auto)
View(auto)
view(toks_auto)
head(toks_auto)
print(toks_auto)
###Keywords
#Set keywords
keywords_opp <- c("incentive", "subsidy", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid", "zero emissions")
# Create a Keyword-In-Context object for each keyword
keyword_freq_opp <- sapply(keywords_opp, function(keyword) {
sum(toks_auto == keyword)
})
print(keyword_freq_opp)
##Make dfm
tokens_auto <- tokens(auto_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove = stopwords("english"), lowercase = TRUE)
##Make dfm
# Convert the corpus to tokens
tokens_auto <- tokens(auto_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
# Remove stopwords from the tokens
tokens_auto <- tokens_remove(tokens_auto, stopwords("english"))
###Keywords
#Set keywords
keywords_opp <- c("incentive", "subsidy", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid", "zero emissions")
###Keywords
#Set keywords
keywords_opp <- c("incentive", "subsidy", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid", "emissions")
# Create a Keyword-In-Context object for each keyword
keyword_freq_opp <- sapply(keywords_opp, function(keyword) {
sum(tokens_auto == keyword)
})
print(keyword_freq_opp)
dfm_auto <- dfm(tokens_auto)
tstat_freq <- textstat_frequency(dfm_auto, n = 5)
head(tstat_freq, 20)
tstat_freq <- textstat_frequency(dfm_auto, n = 20)
head(tstat_freq, 20)
#Calculate overall frequency
keyword__opp_freq <- textstat_frequency(dfm_auto, pattern = keywords_opp)
keyword_freq <- textstat_frequency(tokens_auto, pattern = keywords_to_search)
keyword_freq <- textstat_frequency(tokens_auto, pattern = keywords_opp)
keyword_freq <- colSums(dfm_auto[, keywords_opp])
print(keyword_freq)
keyword_opp_freq <- colSums(dfm_auto[, keywords_opp])
keywords_reg <- c("standard", "rule", "efficieny", "CAFE", "fuel", "harmonization", "implement", "regulation", "monitor", "price", "target", "restriction")
keyword_reg_freq <- colSums(dfm_auto[, keywords_reg])
keywords_reg <- c("standard", "rule", "efficieny", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price", "target", "restriction")
keyword_opp_freq <- colSums(dfm_auto[, keywords_opp])
keyword_reg_freq <- colSums(dfm_auto[, keywords_reg])
matching_columns <- which(colnames(dfm_auto) %in% keywords_reg)
print(matching_columns)
matching_keywords <- colnames(dfm_auto)[colnames(dfm_auto) %in% keywords_reg]
print(matching_keywords)
keywords_reg <- c("standard", "rule", "efficiency", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price")
keyword_reg_freq <- colSums(dfm_auto[, keywords_reg])
print(keyword_reg_freq)
keywords_phy <- c("greenhouse", "water", "climate change", "general", "global warming", "heat", "environment", "land", "nature", "air", "pollution", "flood" )
> matching_columns <- which(colnames(dfm_auto) %in% keywords_phy)
matching_columns <- which(colnames(dfm_auto) %in% keywords_phy)
print(matching_columns)
matching_keywords <- colnames(dfm_auto)[colnames(dfm_auto) %in% keywords_phy]
print(matching_keywords)
keywords_phy <- c("greenhouse", "water", "climate", "general", "warming", "environment", "land", "air", "pollution", "flood" )
keywords_phy <- c("greenhouse", "water", "climate", "general", "warming", "environment", "land", "air", "pollution", "flood" )matching_keywords <- colnames(dfm_auto)[colnames(dfm_auto) %in% keywords_phy]
matching_keywords <- colnames(dfm_auto)[colnames(dfm_auto) %in% keywords_phy]
print(matching_keywords)
keyword_phy_freq <- colSums(dfm_auto[, keywords_phy])
print(keyword_phy_freq)
###Keywords
#Set keywords
keywords_opp <- c("incentive", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid")
keywords_reg <- c("standard", "rule", "efficiency", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price")
keywords_phy <- c("greenhouse", "water", "climate", "general", "warming", "environment", "land", "air", "pollution", "flood" )
#Calculate overall frequency to check
keyword_opp_freq <- colSums(dfm_auto[, keywords_opp])
keyword_reg_freq <- colSums(dfm_auto[, keywords_reg])
keyword_phy_freq <- colSums(dfm_auto[, keywords_phy])
###Create sub-corpus for above/below average for each exposure measure
##Opportunity
# Get the document variable from the corpus
opp <- docvars(auto_corpus, "op_expo_ew")
# Calculate the mean of the document variable
mean_opp <- mean(opp)
# Create a logical condition for grouping
condition_opp <- opp > mean_opp  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_oppmean_corpus <- subset(auto_corpus, condition_opp)
below_oppmean_corpus <- subset(auto_corpus, !condition)
below_oppmean_corpus <- subset(auto_corpus, !condition_opp)
#Make opp dfm
# Convert the corpus to tokens
tokens_opp_above <- tokens(above_oppmean_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_above <- dfm(tokens_opp_above)
tstat_freq <- textstat_frequency(dfm_opp_above, n = 20)
head(tstat_freq, 20)
tokens_opp_below <- tokens(below_oppmean_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_below <- dfm(tokens_opp_below)
##Keywords
#Set opp keywords
keywords_opp <- c("incentive", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid")
#Calculate overall frequency to check
keyword_opp_above <- colSums(dfm_opp_above[, keywords_opp])
print(keyword_opp_above)
keyword_opp_below <- colSums(dfm_opp_below[, keywords_opp])
print(keyword_opp_below)
# Calculate the mean of the document variable
med_opp <- median(opp)
# Create a logical condition for grouping
condition_opp <- opp > med_opp  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_oppmed_corpus <- subset(auto_corpus, condition_opp)
below_oppmed_corpus <- subset(auto_corpus, !condition_opp)
#Make opp dfm
# Convert the corpus to tokens
tokens_opp_above <- tokens(above_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_above <- dfm(tokens_opp_above)
tokens_opp_below <- tokens(below_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_below <- dfm(tokens_opp_below)
##Keywords
#Set opp keywords
keywords_opp <- c("incentive", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid")
#Calculate frequency for above/below mean
keyword_opp_above <- colSums(dfm_opp_above[, keywords_opp])
keyword_opp_below <- colSums(dfm_opp_below[, keywords_opp])
print(keyword_opp_above)
print(keyword_opp_below)
# Create a logical condition for grouping
condition_opp <- opp >= med_opp  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_oppmed_corpus <- subset(auto_corpus, condition_opp)
below_oppmed_corpus <- subset(auto_corpus, !condition_opp)
#Make opp dfm
# Convert the corpus to tokens
tokens_opp_above <- tokens(above_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_above <- dfm(tokens_opp_above)
tokens_opp_below <- tokens(below_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_below <- dfm(tokens_opp_below)
##Keywords
#Set opp keywords
keywords_opp <- c("incentive", "tax", "charging", "technology", "electric", "fund", "research", "development", "battery", "hybrid")
#Calculate frequency for above/below mean
keyword_opp_above <- colSums(dfm_opp_above[, keywords_opp])
keyword_opp_below <- colSums(dfm_opp_below[, keywords_opp])
print(keyword_opp_above)
print(keyword_opp_below)
##Regulatory
# Get the document variable from the corpus
reg <- docvars(auto_corpus, "rg_expo_ew")
# Calculate the mean of the document variable
med_reg <- median(reg)
# Create a logical condition for grouping
condition_reg <- reg >= med_reg  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_regmed_corpus <- subset(auto_corpus, condition_reg)
below_regmed_corpus <- subset(auto_corpus, !condition_reg)
#Make opp dfm
# Convert the corpus to tokens
tokens_reg_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_above <- dfm(tokens_reg_above)
tokens_reg_below <- tokens(below_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_below <- dfm(tokens_reg_below)
##Keywords
#Set opp keywords
keywords_reg <- c("standard", "rule", "efficiency", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price")
#Calculate frequency for above/below mean
keyword_reg_above <- colSums(dfm_reg_above[, keywords_reg])
keyword_reg_below <- colSums(dfm_reg_below[, keywords_reg])
print(keyword_reg_above)
matching_keywords <- colnames(dfm_reg_below)[colnames(dfm_reg_below) %in% keywords_reg]
print(matching_keywords)
# Create a logical condition for grouping
condition_reg <- reg > med_reg  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_regmed_corpus <- subset(auto_corpus, condition_reg)
below_regmed_corpus <- subset(auto_corpus, !condition_reg)
#Make opp dfm
# Convert the corpus to tokens
tokens_reg_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_above <- dfm(tokens_reg_above)
tokens_reg_below <- tokens(below_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_below <- dfm(tokens_reg_below)
##Keywords
#Set opp keywords
keywords_reg <- c("standard", "rule", "efficiency", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price")
#Calculate frequency for above/below mean
keyword_reg_above <- colSums(dfm_reg_above[, keywords_reg])
keyword_reg_below <- colSums(dfm_reg_below[, keywords_reg])
print(keyword_reg_above)
print(keyword_reg_below)
med(reg)
view(med_reg)
mean(reg)
# Calculate the mean of the document variable
med_reg <- mean(reg)
# Create a logical condition for grouping
condition_reg <- reg > med_reg  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_regmed_corpus <- subset(auto_corpus, condition_reg)
below_regmed_corpus <- subset(auto_corpus, !condition_reg)
#Make opp dfm
# Convert the corpus to tokens
tokens_reg_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_above <- dfm(tokens_reg_above)
tokens_reg_below <- tokens(below_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_below <- dfm(tokens_reg_below)
##Keywords
#Set opp keywords
keywords_reg <- c("standard", "rule", "efficiency", "cafe", "fuel", "harmonization", "implement", "regulation", "monitor", "price")
#Calculate frequency for above/below mean
keyword_reg_above <- colSums(dfm_reg_above[, keywords_reg])
keyword_reg_below <- colSums(dfm_reg_below[, keywords_reg])
print(keyword_reg_above)
print(keyword_reg_below)
##Physical
# Get the document variable from the corpus
phy <- docvars(auto_corpus, "ph_expo_ew")
# Calculate the mean of the document variable
med_phy <- median(phy)
# Create a logical condition for grouping
condition_phy <- phy > med_phy  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_phymed_corpus <- subset(auto_corpus, condition_phy)
below_phymed_corpus <- subset(auto_corpus, !condition_phy)
#Make opp dfm
# Convert the corpus to tokens
tokens_phy_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
#Make opp dfm
# Convert the corpus to tokens
tokens_phy_above <- tokens(above_phymed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_phy_above <- dfm(tokens_phy_above)
tokens_phy_below <- tokens(below_phymed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_phy_below <- dfm(tokens_phy_below)
##Keywords
#Set opp keywords
keywords_phy <- c("greenhouse", "water", "climate", "general", "warming", "environment", "land", "air", "pollution", "flood" )
#Calculate frequency for above/below mean
keyword_phy_above <- colSums(dfm_phy_above[, keywords_phy])
keyword_reg_below <- colSums(dfm_reg_below[, keywords_phy])
keyword_phy_below <- colSums(dfm_phy_below[, keywords_phy])
matching_keywords_phy <- colnames(dfm_phy_above)[colnames(dfm_phy_above) %in% keywords_phy]
print(matching_keywords_phy)
print(keyword_phy_below)
keywords_phy_above <- c("greenhouse", "water", "climate", "general", "warming", "environment", "land", "air", "pollution")
#Calculate frequency for above/below mean
keyword_phy_above <- colSums(dfm_phy_above[, keywords_phy_above])
print(keyword_phy_above)
install.packages("quanteda.textstats", "wordcloud")
install.packages("quanteda.textstats", "wordcloud")
# Load packages
pacman::p_load(tidyverse, readxl, quanteda, quanteda.textstats, stm, wordcloud)
###Try wordlcloud approach
set.seed(100)
textplot_wordcloud(dfm_opp_above, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
rm(list=ls())
# Load packages
pacman::p_load(tidyverse, readxl, quanteda, quanteda.textstats, stm, wordcloud)
# Set working directory
if(Sys.info()["user"]=="fiona" ) {setwd("/Users/fiona/Dropbox (Princeton)/BBH/BBH1/")}
#Load data
auto <- read_excel("data/03_final/issues_texts/auto.xlsx")
#Create climate binary variable
auto$CLI <- grepl("ENV|CAW|ENG|FUE", auto$issue_code)
#Filter data for climate lobbying only
auto <- auto %>%
filter(CLI == "TRUE")
auto <- auto %>%
filter(
complete.cases(
op_expo_ew, rg_expo_ew, ph_expo_ew
)
)
###Create overall corpus
auto_corpus <-corpus(auto$issue_text, docnames = seq_len(nrow(auto)))
# Add document-level information
docvars(auto_corpus) <- auto[, c("gvkey", "conm", "year", "op_expo_ew", "rg_expo_ew", "ph_expo_ew")]
###Create sub-corpus for above/below average for each exposure measure
##Opportunity
# Get the document variable from the corpus
opp <- docvars(auto_corpus, "op_expo_ew")
# Calculate the mean of the document variable
med_opp <- median(opp)
# Create a logical condition for grouping
condition_opp <- opp >= med_opp  # or < if you want the opposite comparison
# Create a logical condition for grouping
condition_opp <- opp > med_opp  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_oppmed_corpus <- subset(auto_corpus, condition_opp)
below_oppmed_corpus <- subset(auto_corpus, !condition_opp)
##Regulatory
# Get the document variable from the corpus
reg <- docvars(auto_corpus, "rg_expo_ew")
# Calculate the mean of the document variable
med_reg <- median(reg)
# Create a logical condition for grouping
condition_reg <- reg > med_reg  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_regmed_corpus <- subset(auto_corpus, condition_reg)
below_regmed_corpus <- subset(auto_corpus, !condition_reg)
#Make opp dfm
# Convert the corpus to tokens
tokens_reg_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_above <- dfm(tokens_reg_above)
tokens_reg_below <- tokens(below_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_reg_below <- dfm(tokens_reg_below)
##Physical
# Get the document variable from the corpus
phy <- docvars(auto_corpus, "ph_expo_ew")
# Calculate the mean of the document variable
med_phy <- median(phy)
# Create a logical condition for grouping
condition_phy <- phy > med_phy  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_phymed_corpus <- subset(auto_corpus, condition_phy)
below_phymed_corpus <- subset(auto_corpus, !condition_phy)
###Try wordlcloud approach
set.seed(100)
textplot_wordcloud(dfm_opp_above, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
install.packages("quanteda.textstats", "wordcloud", "quanteda.textplots")
install.packages("quanteda.textstats", "wordcloud", "quanteda.textplots")
# Load packages
pacman::p_load(tidyverse, readxl, quanteda, quanteda.textstats, stm, wordcloud, quanteda.textplots)
textplot_wordcloud(dfm_opp_above, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
#Load data
auto <- read_excel("data/03_final/issues_texts/auto.xlsx")
#Create climate binary variable
auto$CLI <- grepl("ENV|CAW|ENG|FUE", auto$issue_code)
#Filter data for climate lobbying only
auto <- auto %>%
filter(CLI == "TRUE")
auto <- auto %>%
filter(
complete.cases(
op_expo_ew, rg_expo_ew, ph_expo_ew
)
)
###Create overall corpus
auto_corpus <-corpus(auto$issue_text, docnames = seq_len(nrow(auto)))
# Add document-level information
docvars(auto_corpus) <- auto[, c("gvkey", "conm", "year", "op_expo_ew", "rg_expo_ew", "ph_expo_ew")]
###Create sub-corpus for above/below average for each exposure measure
##Opportunity
# Get the document variable from the corpus
opp <- docvars(auto_corpus, "op_expo_ew")
# Calculate the mean of the document variable
med_opp <- median(opp)
# Create a logical condition for grouping
condition_opp <- opp > med_opp  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_oppmed_corpus <- subset(auto_corpus, condition_opp)
below_oppmed_corpus <- subset(auto_corpus, !condition_opp)
#Make opp dfm
# Convert the corpus to tokens
tokens_opp_above <- tokens(above_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_above <- dfm(tokens_opp_above)
tokens_opp_below <- tokens(below_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
dfm_opp_below <- dfm(tokens_opp_below)
textplot_wordcloud(dfm_opp_above, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
#Make opp dfm
# Convert the corpus to tokens
tokens_opp_above <- tokens(above_oppmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE, remove = stopwords("english"))
tokens_opp_above <- tokens_select(tokens_opp_above,
pattern = stopwords("english"),
selection = "remove")
dfm_opp_above <- dfm(tokens_opp_above)
textplot_wordcloud(dfm_opp_above, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
tokens_opp_below <- tokens_select(tokens_opp_below,
pattern = stopwords("english"),
selection = "remove")
dfm_opp_below <- dfm(tokens_opp_below)
textplot_wordcloud(dfm_opp_below, min_count = 6, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(dfm_opp_below, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(dfm_opp_above, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
##Regulatory
# Get the document variable from the corpus
reg <- docvars(auto_corpus, "rg_expo_ew")
# Calculate the mean of the document variable
med_reg <- median(reg)
# Create a logical condition for grouping
condition_reg <- reg > med_reg  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_regmed_corpus <- subset(auto_corpus, condition_reg)
below_regmed_corpus <- subset(auto_corpus, !condition_reg)
#Make opp dfm
# Convert the corpus to tokens
tokens_reg_above <- tokens(above_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
tokens_reg_above <- tokens_select(tokens_reg_above,
pattern = stopwords("english"),
selection = "remove")
dfm_reg_above <- dfm(tokens_reg_above)
tokens_reg_below <- tokens(below_regmed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
tokens_reg_below <- tokens_select(tokens_reg_below,
pattern = stopwords("english"),
selection = "remove")
dfm_reg_below <- dfm(tokens_reg_below)
textplot_wordcloud(dfm_reg_above, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(dfm_reg_below, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
##Physical
# Get the document variable from the corpus
phy <- docvars(auto_corpus, "ph_expo_ew")
# Calculate the mean of the document variable
med_phy <- median(phy)
# Create a logical condition for grouping
condition_phy <- phy > med_phy  # or < if you want the opposite comparison
# Split the corpus into two groups based on the condition
above_phymed_corpus <- subset(auto_corpus, condition_phy)
below_phymed_corpus <- subset(auto_corpus, !condition_phy)
#Make opp dfm
# Convert the corpus to tokens
tokens_phy_above <- tokens(above_phymed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
tokens_phy_above <- tokens_select(tokens_phy_above,
pattern = stopwords("english"),
selection = "remove")
dfm_phy_above <- dfm(tokens_phy_above)
tokens_phy_below <- tokens(below_phymed_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, lowercase = TRUE)
tokens_phy_below <- tokens_select(tokens_phy_below,
pattern = stopwords("english"),
selection = "remove")
dfm_phy_below <- dfm(tokens_phy_below)
cloud_phy_above <- textplot_wordcloud(dfm_phy_above, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
cloud_phy_below <-textplot_wordcloud(dfm_phy_below, min_count = 8, random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
#Topic models
set.seed(1234)
if (require(stm)) {
my_lda_fit20 <- stm(dfm_opp_above, K = 20, verbose = FALSE)
plot(my_lda_fit20)
}
if (require(stm)) {
my_lda_fit20 <- stm(dfm_opp_above, K = 5, verbose = FALSE)
plot(my_lda_fit20)
}
